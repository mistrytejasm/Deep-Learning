{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648f87e8",
   "metadata": {},
   "source": [
    "RNN using PyTorch | Question Answering System using PyTorch | Video 13 | CampusX\n",
    "\n",
    "https://www.youtube.com/watch?v=xjzWrPQ66VQ&list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7&index=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06bdfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee27139c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Dataset/100_Unique_QA_Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33d51346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize Words \n",
    "def tokenize(text):\n",
    "  text = text.lower()\n",
    "  text = text.replace(\"?\",\"\")\n",
    "  text = text.replace(\"'\",\"\")\n",
    "  return text.split()\n",
    "\n",
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "127d679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A words Vocabulari (find total unique words in dataset)\n",
    "# first create a dictionay so we can store each unique word with it index\n",
    "\n",
    "vocab = {'<UNK>':0}\n",
    "\n",
    "def build_vocab(row):\n",
    "  tokenized_question = tokenize(row['question'])\n",
    "  tokenized_answer = tokenize(row['answer'])\n",
    "\n",
    "  merged_tokens = tokenized_question + tokenized_answer\n",
    "\n",
    "  for token in merged_tokens:\n",
    "    if token not in vocab:\n",
    "      vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2c13f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dce5016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'what': 1, 'is': 2, 'the': 3, 'capital': 4, 'of': 5, 'france': 6, 'paris': 7, 'germany': 8, 'berlin': 9, 'who': 10, 'wrote': 11, 'to': 12, 'kill': 13, 'a': 14, 'mockingbird': 15, 'harper-lee': 16, 'largest': 17, 'planet': 18, 'in': 19, 'our': 20, 'solar': 21, 'system': 22, 'jupiter': 23, 'boiling': 24, 'point': 25, 'water': 26, 'celsius': 27, '100': 28, 'painted': 29, 'mona': 30, 'lisa': 31, 'leonardo-da-vinci': 32, 'square': 33, 'root': 34, '64': 35, '8': 36, 'chemical': 37, 'symbol': 38, 'for': 39, 'gold': 40, 'au': 41, 'which': 42, 'year': 43, 'did': 44, 'world': 45, 'war': 46, 'ii': 47, 'end': 48, '1945': 49, 'longest': 50, 'river': 51, 'nile': 52, 'japan': 53, 'tokyo': 54, 'developed': 55, 'theory': 56, 'relativity': 57, 'albert-einstein': 58, 'freezing': 59, 'fahrenheit': 60, '32': 61, 'known': 62, 'as': 63, 'red': 64, 'mars': 65, 'author': 66, '1984': 67, 'george-orwell': 68, 'currency': 69, 'united': 70, 'kingdom': 71, 'pound': 72, 'india': 73, 'delhi': 74, 'discovered': 75, 'gravity': 76, 'newton': 77, 'how': 78, 'many': 79, 'continents': 80, 'are': 81, 'there': 82, 'on': 83, 'earth': 84, '7': 85, 'gas': 86, 'do': 87, 'plants': 88, 'use': 89, 'photosynthesis': 90, 'co2': 91, 'smallest': 92, 'prime': 93, 'number': 94, '2': 95, 'invented': 96, 'telephone': 97, 'alexander-graham-bell': 98, 'australia': 99, 'canberra': 100, 'ocean': 101, 'pacific-ocean': 102, 'speed': 103, 'light': 104, 'vacuum': 105, '299,792,458m/s': 106, 'language': 107, 'spoken': 108, 'brazil': 109, 'portuguese': 110, 'penicillin': 111, 'alexander-fleming': 112, 'canada': 113, 'ottawa': 114, 'mammal': 115, 'whale': 116, 'element': 117, 'has': 118, 'atomic': 119, '1': 120, 'hydrogen': 121, 'tallest': 122, 'mountain': 123, 'everest': 124, 'city': 125, 'big': 126, 'apple': 127, 'newyork': 128, 'planets': 129, 'starry': 130, 'night': 131, 'vangogh': 132, 'formula': 133, 'h2o': 134, 'italy': 135, 'rome': 136, 'country': 137, 'famous': 138, 'sushi': 139, 'was': 140, 'first': 141, 'person': 142, 'step': 143, 'moon': 144, 'armstrong': 145, 'main': 146, 'ingredient': 147, 'guacamole': 148, 'avocado': 149, 'sides': 150, 'does': 151, 'hexagon': 152, 'have': 153, '6': 154, 'china': 155, 'yuan': 156, 'pride': 157, 'and': 158, 'prejudice': 159, 'jane-austen': 160, 'iron': 161, 'fe': 162, 'hardest': 163, 'natural': 164, 'substance': 165, 'diamond': 166, 'continent': 167, 'by': 168, 'area': 169, 'asia': 170, 'president': 171, 'states': 172, 'george-washington': 173, 'bird': 174, 'its': 175, 'ability': 176, 'mimic': 177, 'sounds': 178, 'parrot': 179, 'longest-running': 180, 'animated': 181, 'tv': 182, 'show': 183, 'simpsons': 184, 'vaticancity': 185, 'most': 186, 'moons': 187, 'saturn': 188, 'romeo': 189, 'juliet': 190, 'shakespeare': 191, 'earths': 192, 'atmosphere': 193, 'nitrogen': 194, 'bones': 195, 'adult': 196, 'human': 197, 'body': 198, '206': 199, 'metal': 200, 'liquid': 201, 'at': 202, 'room': 203, 'temperature': 204, 'mercury': 205, 'russia': 206, 'moscow': 207, 'electricity': 208, 'benjamin-franklin': 209, 'second-largest': 210, 'land': 211, 'color': 212, 'ripe': 213, 'banana': 214, 'yellow': 215, 'month': 216, '28': 217, 'days': 218, 'common': 219, 'february': 220, 'study': 221, 'living': 222, 'organisms': 223, 'called': 224, 'biology': 225, 'home': 226, 'great': 227, 'wall': 228, 'bees': 229, 'collect': 230, 'from': 231, 'flowers': 232, 'nectar': 233, 'opposite': 234, 'day': 235, 'south': 236, 'korea': 237, 'seoul': 238, 'bulb': 239, 'edison': 240, 'humans': 241, 'breathe': 242, 'survival': 243, 'oxygen': 244, '144': 245, '12': 246, 'pyramids': 247, 'giza': 248, 'egypt': 249, 'sea': 250, 'creature': 251, 'eight': 252, 'arms': 253, 'octopus': 254, 'holiday': 255, 'celebrated': 256, 'december': 257, '25': 258, 'christmas': 259, 'yen': 260, 'legs': 261, 'spider': 262, 'sport': 263, 'uses': 264, 'net,': 265, 'ball,': 266, 'hoop': 267, 'basketball': 268, 'kangaroos': 269, 'female': 270, 'minister': 271, 'uk': 272, 'margaretthatcher': 273, 'fastest': 274, 'animal': 275, 'cheetah': 276, 'periodic': 277, 'table': 278, 'spain': 279, 'madrid': 280, 'closest': 281, 'sun': 282, 'father': 283, 'computers': 284, 'charlesbabbage': 285, 'mexico': 286, 'mexicocity': 287, 'colors': 288, 'rainbow': 289, 'musical': 290, 'instrument': 291, 'black': 292, 'white': 293, 'keys': 294, 'piano': 295, 'americas': 296, '1492': 297, 'christophercolumbus': 298, 'disney': 299, 'character': 300, 'long': 301, 'nose': 302, 'grows': 303, 'it': 304, 'when': 305, 'lying': 306, 'pinocchio': 307, 'directed': 308, 'movie': 309, 'titanic': 310, 'jamescameron': 311, 'superhero': 312, 'also': 313, 'dark': 314, 'knight': 315, 'batman': 316, 'brasilia': 317, 'fruit': 318, 'king': 319, 'fruits': 320, 'mango': 321, 'eiffel': 322, 'tower': 323}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9637c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f077af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to numerical indices\n",
    "\n",
    "def text_to_indices(text, vocab):\n",
    "\n",
    "  indexed_text = []\n",
    "\n",
    "  for token in tokenize(text):\n",
    "    if token in vocab:\n",
    "      indexed_text.append(vocab[token])\n",
    "    else:\n",
    "      indexed_text.append(vocab['<UNK>'])\n",
    "  \n",
    "  return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d1d64f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"france\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3a3cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89b4d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "\n",
    "  def __init__(self, df, vocab):\n",
    "    self.df = df\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    numerical_question = text_to_indices(self.df.iloc[index]['question'], self.vocab)\n",
    "    numerical_answer = text_to_indices(self.df.iloc[index]['answer'], self.vocab)\n",
    "\n",
    "    return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51b2f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0971de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5, 6]), tensor([7]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94a9355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93e5a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n"
     ]
    }
   ],
   "source": [
    "for question, answer in dataloader:\n",
    "  print(question, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487f1a5",
   "metadata": {},
   "source": [
    "## Now Design Our RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5d339e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e2da851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
    "    self.rnn = nn.RNN(50, 64, batch_first=True)\n",
    "    self.fc = nn.Linear(64, vocab_size)\n",
    "\n",
    "  def forward(self, question):\n",
    "    embedded_question = self.embedding(question)\n",
    "    hidden, final = self.rnn(embedded_question)\n",
    "    output = self.fc(final.squeeze(0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d6cc25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a: torch.Size([1, 6])\n",
      "shape of b: torch.Size([1, 6, 50])\n",
      "shape of c: torch.Size([1, 6, 64])\n",
      "shape of d: torch.Size([1, 1, 64])\n",
      "shape of e: torch.Size([1, 324])\n"
     ]
    }
   ],
   "source": [
    "x = nn.Embedding(324, embedding_dim=50)\n",
    "y = nn.RNN(50, 64, batch_first=True)\n",
    "z = nn.Linear(64, 324)\n",
    "\n",
    "a = dataset[0][0].reshape(1,6)\n",
    "print(\"shape of a:\", a.shape)\n",
    "b = x(a)\n",
    "print(\"shape of b:\", b.shape)\n",
    "c, d = y(b)\n",
    "print(\"shape of c:\", c.shape)\n",
    "print(\"shape of d:\", d.shape)\n",
    "\n",
    "e = z(d.squeeze(0))\n",
    "\n",
    "print(\"shape of e:\", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a6ba9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "252feaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b72e0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e620fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71a746e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 521.289861\n",
      "Epoch: 2, Loss: 449.226931\n",
      "Epoch: 3, Loss: 372.449696\n",
      "Epoch: 4, Loss: 314.276651\n",
      "Epoch: 5, Loss: 264.359702\n",
      "Epoch: 6, Loss: 216.572748\n",
      "Epoch: 7, Loss: 172.396774\n",
      "Epoch: 8, Loss: 134.442740\n",
      "Epoch: 9, Loss: 103.066869\n",
      "Epoch: 10, Loss: 78.863242\n",
      "Epoch: 11, Loss: 60.929637\n",
      "Epoch: 12, Loss: 47.640117\n",
      "Epoch: 13, Loss: 37.761083\n",
      "Epoch: 14, Loss: 30.687868\n",
      "Epoch: 15, Loss: 25.273892\n",
      "Epoch: 16, Loss: 21.259196\n",
      "Epoch: 17, Loss: 18.100846\n",
      "Epoch: 18, Loss: 15.408927\n",
      "Epoch: 19, Loss: 13.383157\n",
      "Epoch: 20, Loss: 11.604631\n",
      "Epoch: 21, Loss: 10.264140\n",
      "Epoch: 22, Loss: 9.012790\n",
      "Epoch: 23, Loss: 7.933586\n",
      "Epoch: 24, Loss: 7.114678\n",
      "Epoch: 25, Loss: 6.353780\n",
      "Epoch: 26, Loss: 5.740671\n",
      "Epoch: 27, Loss: 5.186834\n",
      "Epoch: 28, Loss: 4.701916\n",
      "Epoch: 29, Loss: 4.291141\n",
      "Epoch: 30, Loss: 3.916531\n",
      "Epoch: 31, Loss: 3.590623\n",
      "Epoch: 32, Loss: 3.299019\n",
      "Epoch: 33, Loss: 3.047155\n",
      "Epoch: 34, Loss: 2.815495\n",
      "Epoch: 35, Loss: 2.610094\n",
      "Epoch: 36, Loss: 2.417628\n",
      "Epoch: 37, Loss: 2.252792\n",
      "Epoch: 38, Loss: 2.095829\n",
      "Epoch: 39, Loss: 1.958294\n",
      "Epoch: 40, Loss: 1.833344\n",
      "Epoch: 41, Loss: 1.708471\n",
      "Epoch: 42, Loss: 1.602601\n",
      "Epoch: 43, Loss: 1.501509\n",
      "Epoch: 44, Loss: 1.408769\n",
      "Epoch: 45, Loss: 1.325599\n",
      "Epoch: 46, Loss: 1.243864\n",
      "Epoch: 47, Loss: 1.170463\n",
      "Epoch: 48, Loss: 1.104075\n",
      "Epoch: 49, Loss: 1.043036\n",
      "Epoch: 50, Loss: 0.982461\n",
      "Epoch: 51, Loss: 0.925690\n",
      "Epoch: 52, Loss: 0.877845\n",
      "Epoch: 53, Loss: 0.827839\n",
      "Epoch: 54, Loss: 0.782702\n",
      "Epoch: 55, Loss: 0.740240\n",
      "Epoch: 56, Loss: 0.701993\n",
      "Epoch: 57, Loss: 0.664537\n",
      "Epoch: 58, Loss: 0.629853\n",
      "Epoch: 59, Loss: 0.597534\n",
      "Epoch: 60, Loss: 0.567205\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    "  for question, answer in dataloader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(question)\n",
    "\n",
    "    # loss -> output shape (1,324) - (1)\n",
    "    loss = criterion(output, answer[0])\n",
    "\n",
    "    # gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch+1}, Loss: {total_loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold=0.5):\n",
    "\n",
    "  # convert question to numbers\n",
    "  numerical_question = text_to_indices(question, vocab)\n",
    "\n",
    "  # tensor\n",
    "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  output = model(question_tensor)\n",
    "\n",
    "  # convert logits to probs\n",
    "  probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "  # find index of max prob\n",
    "  value, index = torch.max(probs, dim=1)\n",
    "\n",
    "  if value <  threshold: \n",
    "    print(\"I dont Know\")\n",
    "\n",
    "  print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c53ce61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter Your Question: \")\n",
    "predict(model, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77d707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
